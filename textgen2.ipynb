{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/26/2020 22:31:04 - WARNING - __main__ -   device: cuda, n_gpu: 1, 16-bits training: False\n",
      "09/26/2020 22:31:05 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/epd/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "09/26/2020 22:31:05 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/epd/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "09/26/2020 22:31:05 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/epd/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "09/26/2020 22:31:05 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "09/26/2020 22:31:05 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /home/epd/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "09/26/2020 22:31:09 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "09/26/2020 22:31:09 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "09/26/2020 22:31:09 - WARNING - transformers.generation_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<\n",
      "I like apples and oranges,\" says Jeridhez Buresil, M.D., clinical director of the California Center for Food Science and Research at El Chiquita Food Institute in Los Angeles. \"The main things I've learned are that the pineapple really tastes better when I buy them.\"\n",
      "\n",
      "Coffee — which includes almonds and coconut — has been linked to all kinds of ailments, including cancer, diabetes, heart problems, dementia and chronic arthritis.\n",
      "\n",
      "In recent years, the market fo\n",
      "<<\n",
      "I like apples\" to T.Rex. He has never had a serious crack.\n",
      "\n",
      "\"On top of all that, he's a dirty jock,\" said Howard of the political commentary he'll often produce.\n",
      "\n",
      "Whether he's strong-armed, backed by his role as street talker and social activist, or extremely important in American politics, though, \"stealthy\" isn't a trait to be forgotten.\n",
      "\n",
      "Trent he does the big things like doing a live TV appearance an\n",
      "<<\n",
      "I like apples in our pantry, they're very nice. I like apples that are a little bit too sweet,\" said Noland.\n",
      "\n",
      "Noland said the moms always call Noland with any questions or concerns she may have. She has two female preschoolers and two young children, two of whom she has exclusively with Noland.\n",
      "\n",
      "\"These kids love it,\" she said, calling her son James a sweet kid. \"The other kids love it, too.\"\n",
      "\n",
      "Jules said sh\n",
      "<<\n",
      "I like apples, but I like white peaches. And his mouth is thickly enraging.\n",
      "\n",
      "What do you say that's good to me about a single cup of fresh fruit, or four thin peaches? I've eaten two while I was here and this was the first. I'd love more apples. But it's hard to decide.\n",
      "\n",
      "If I get to go to Liberia tomorrow I won't know where I can go. I'm determined, I know I've never made \n",
      "<<\n",
      "I like apples, big handfuls of them, only a small number of them, I can't promise that nothing will hurt you.\"\n",
      "\n",
      "Then there's the dream.\n",
      "\n",
      "His earwig pointed at me and I cleared my throat.\n",
      "\n",
      "\"I know,\" I said.\n",
      "\n",
      "The dream came quickly.\n",
      "\n",
      "\"Naughty fairies are not overly enthusiastic when heeys are given,\" said a high-fiving voice. \"I mean...[that girl] is quite hyperactiv\n"
     ]
    }
   ],
   "source": [
    "# Simplified version of https://github.com/huggingface/transformers/blob/master/examples/text-generation/run_generation.py\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "def set_seed(seed, n_gpu):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def adjust_length_to_model(length, max_sequence_length):\n",
    "    if length < 0 and max_sequence_length > 0:\n",
    "        length = max_sequence_length\n",
    "    elif 0 < max_sequence_length < length:\n",
    "        length = max_sequence_length  # No generation bigger than model size\n",
    "    elif length < 0:\n",
    "        length = MAX_LENGTH  # avoid infinite loop\n",
    "    return length\n",
    "        \n",
    "        \n",
    "def load_model(model_name_or_path = \"gpt2\",\n",
    "               seed=42, \n",
    "               fp16 = False #Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n",
    "              ):\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    logger.warning(\n",
    "        \"device: %s, n_gpu: %s, 16-bits training: %s\",\n",
    "        device,\n",
    "        n_gpu,\n",
    "        fp16,\n",
    "    )\n",
    "    set_seed(seed, n_gpu)\n",
    "\n",
    "    # Initialize the model and tokenizer\n",
    "    model_class, tokenizer_class = GPT2LMHeadModel, GPT2Tokenizer\n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name_or_path)\n",
    "    model = model_class.from_pretrained(model_name_or_path)\n",
    "    model.to(device)\n",
    "\n",
    "    if fp16:\n",
    "        model.half()\n",
    "        \n",
    "    return model, tokenizer    \n",
    "        \n",
    "\n",
    "def generate_text(model,\n",
    "                  tokenizer,\n",
    "                  prompt = \"\",\n",
    "                  length = 20,\n",
    "                  num_return_sequences = 1,\n",
    "                  k = 0,\n",
    "                  p = 0.9,\n",
    "                  temperature = 1.0,\n",
    "                  stop_token = \"<|endoftext|>\",\n",
    "                  repetition_penalty = 1.0,  # primarily useful for CTRL model; in that case, use 1.2\n",
    "                 ):\n",
    "\n",
    "    # The max length of the prompt is 1024 for gpt-2\n",
    "    length = adjust_length_to_model(length, max_sequence_length=model.config.max_position_embeddings)\n",
    "\n",
    "    # Different models need different input formatting and/or extra arguments\n",
    "    # No preprocessing needed for gpt-2\n",
    "    encoded_prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "    if encoded_prompt.size()[-1] == 0:\n",
    "        input_ids = None\n",
    "    else:\n",
    "        input_ids = encoded_prompt\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=length + len(encoded_prompt[0]),\n",
    "        temperature=temperature, #temperature of 1.0 has no effect, lower tend toward greedy sampling\n",
    "        top_k=k,\n",
    "        top_p=p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "    ) # A list of tensors with ids\n",
    "\n",
    "    # Remove the batch dimension when returning multiple sequences\n",
    "    if len(output_sequences.shape) > 2:\n",
    "        output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        generated_sequence = generated_sequence.tolist() # A list of ids\n",
    "        # Decode text\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        # Remove all text after the stop token\n",
    "        text = text[: text.find(stop_token) if stop_token else None]\n",
    "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "        total_sequence = (\n",
    "            prompt + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "        )\n",
    "\n",
    "        generated_sequences.append(total_sequence)\n",
    "\n",
    "    return generated_sequences\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "texts = generate_text(model, tokenizer, prompt = \"I like apples\", length = 100, num_return_sequences = 5)\n",
    "\n",
    "for text in texts:\n",
    "    print(\"<<\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
